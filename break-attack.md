# 부수는건 쉬워요!

번역 : http://www.cleverhans.io/security/privacy/ml/2016/12/15/breaking-things-is-easy.html

by Nicolas Papernot and Ian Goodfellow || 번역 : 구글 번역기 + davinnovation

몇 년전만 하더라도, 머신러닝 알고리즘은 물체 인식, 번역 같은 의미있는 작업을 간단하게 수행하지 못했습니다. 머신러닝 알고리즘이 틀리는 것은, 예외라기 보다는 'RULE' 같은 것이였습니다. 오늘날, 머신러닝 알고리즘은 naturally occurring input으로도 human을 능가할 수 있을 정도로 발전했습니다. 머신러닝은 사소한 공격으로도 매우 성능이 떨어지기 때문에 아직은 실제 인간 수준의 실적은 내지 못합니다. 다른 말로는, 기계 학습이 작동하는 지점에 도달했지만 쉽게 깨질 수 있다는 겁니다.

이 포스트에서는 어떤 공격이 머신러닝 알고리즘을 망가트릴 수 있는지 논의할 겁니다. 좀 Academic하게 말해보자면 security and privacy of machine learning입니다. Ian과 Nicolas는 cleverhans 오픈 소스 프로젝트를 만들어 머신러닝의 취약성을 벤치마킹하고 있습니다. 

지금까지는, 대부분의 머신러닝은 아주 약한 위협(threat) 중에서 개발되어 왔습니다. 머신러닝 시스템은 자연스러운 상태에서 작동하도록 디자인 되었던 것이죠. 이제는, 공격적인 환경에서도 작동이 가능한 머신러닝 시스템을 디자인하고자 합니다.

머신러닝 시스템은 학습 과정(learning phase) 중이나, 예측 시(inference phase) 공격이 되었습니다. 공격자들은 모델의 매개 변수와 구조, 모델의 i/o에 대한 모든 액세스를 가지고 있었습니다.

머신러닝 모델을 망가뜨리려면, 공격자는 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)을 손상시켜야 합니다. - 이건 CIA model의 보안 속성들입니다.

- 기밀성을 성취하기 위해서는 머신러닝 시스템은 허락되지 않은 사용자에게 정보를 노출시켜서는 안됩니다. 예를 들어, 연구자가 환자의 의료 기록을 가지고 질병 진달을 할 수 있는 머신러닝 모델을 만들었다고 해봅시다. 이 모델은 의사에게는 중요한 정보를 줄 것이지만, 악의적인 사람이 개인의 의료 데이터를 복구할 수 없도록 해야합니다.
- 모델의 무결성을 조잘할수 있다면, 예측값을 다른 '의도된' 값으로 치환할 수 있습니다. 예를 들어, Spammer가 이메일을 spam filter에 안걸리게 할 수 있는 것 처럼요.
- 공격자는 시스템의 가용성을 손상시킬 수도 있습니다. 자율주행자동차 운행 시, 헷갈리는 물체를 도로에 두어 안전 장치 모드로 강제 전환시킬수도 있겠죠.

물론 위는 모두 가상의 상황입니다...만 지금까지 보안 연구자들이 어떤 종류으 ㅣ공격을 시도했을까요? 대표적인 것 3가지부터 시작해봅시다. 
: Training time 중 무결성 공격, Inference 중 무결성 공격, 그리고 privacy attack

## 트레이닝 데이터 오염시키기

공격자들은 트레이닝 데이터를 수정하거나 데이터를 추가하여 훈련 과정을 방해하여 모델의 무결성을 깨트릴 수 있습니다.