# 부수는건 쉬워요!

번역 : http://www.cleverhans.io/security/privacy/ml/2016/12/15/breaking-things-is-easy.html

by Nicolas Papernot and Ian Goodfellow || 번역 : 구글 번역기 + davinnovation

몇 년전만 하더라도, 머신러닝 알고리즘은 물체 인식, 번역 같은 의미있는 작업을 간단하게 수행하지 못했습니다. 머신러닝 알고리즘이 틀리는 것은, 예외라기 보다는 'RULE' 같은 것이였습니다. 오늘날, 머신러닝 알고리즘은 naturally occurring input으로도 human을 능가할 수 있을 정도로 발전했습니다. 머신러닝은 사소한 공격으로도 매우 성능이 떨어지기 때문에 아직은 실제 인간 수준의 실적은 내지 못합니다. 다른 말로는, 기계 학습이 작동하는 지점에 도달했지만 쉽게 깨질 수 있다는 겁니다.

이 포스트에서는 어떤 공격이 머신러닝 알고리즘을 망가트릴 수 있는지 논의할 겁니다. 좀 Academic하게 말해보자면 security and privacy of machine learning입니다. Ian과 Nicolas는 cleverhans 오픈 소스 프로젝트를 만들어 머신러닝의 취약성을 벤치마킹하고 있습니다. 

지금까지는, 대부분의 머신러닝은 아주 약한 위협(threat) 중에서 개발되어 왔습니다. 머신러닝 시스템은 자연스러운 상태에서 작동하도록 디자인 되었던 것이죠. 이제는, 공격적인 환경에서도 작동이 가능한 머신러닝 시스템을 디자인하고자 합니다.

머신러닝 시스템은 학습 과정(learning phase) 중이나, 예측 시(inference phase) 공격이 되었습니다. 공격자들은 모델의 매개 변수와 구조, 모델의 i/o에 대한 모든 액세스를 가지고 있었습니다.

머신러닝 모델을 망가뜨리려면, 공격자는 기밀성(Confidentiality), 무결성(Integrity), 가용성(Availability)을 손상시켜야 합니다. - 이건 CIA model의 보안 속성들입니다.

- 기밀성을 성취하기 위해서는 머신러닝 시스템은 허락되지 않은 사용자에게 정보를 노출시켜서는 안됩니다. 예를 들어, 연구자가 환자의 의료 기록을 가지고 질병 진달을 할 수 있는 머신러닝 모델을 만들었다고 해봅시다. 이 모델은 의사에게는 중요한 정보를 줄 것이지만, 악의적인 사람이 개인의 의료 데이터를 복구할 수 없도록 해야합니다.
- 모델의 무결성을 조잘할수 있다면, 예측값을 다른 '의도된' 값으로 치환할 수 있습니다. 예를 들어, Spammer가 이메일을 spam filter에 안걸리게 할 수 있는 것 처럼요.
- 공격자는 시스템의 가용성을 손상시킬 수도 있습니다. 자율주행자동차 운행 시, 헷갈리는 물체를 도로에 두어 안전 장치 모드로 강제 전환시킬수도 있겠죠.

물론 위는 모두 가상의 상황입니다...만 지금까지 보안 연구자들이 어떤 종류의 공격을 시도했을까요? 대표적인 것 3가지부터 시작해봅시다. 
: Training time 중 무결성 공격, Inference 중 무결성 공격, 그리고 privacy attack

## 트레이닝 데이터 오염시키기

공격자들은 트레이닝 데이터를 수정하거나 데이터를 추가하여 훈련 과정을 방해하여 모델의 무결성을 깨트릴 수 있습니다. 예를 들어볼까요? 브라운 박사는 코난을 범죄자로 만들고 싶어합니다. 그는 매우 특이한 신발을 준비하여 코난에게 전달합니다. 코난이 계속 경찰 앞에서 전달받은 신발을 신는다면, 경찰은 그 특이한 신발과 코난의 연관성을 강하게 인지할 겁니다. 브라운 박사는 코난에게 준 특이한 신발을 신고 범죄를 저지른 후, 코난이 혐의에 걸릴 수 있는 흔적을 남깁니다.

머신러닝에서도 공격자의 전략은 실제 현실세계에서 작동할 때 예측 오류를 증가시키는 방향으로 혼란을 주는 겁니다. 이 방법은 SVM(support vector machine)에 예측 오류를 증가시키기 위해 사용이 됩니다. 예측 오류를 측정하기 위한 loss function의 convexity는 공격자가 어느 지점을 훼손하면 inference 단계에서 가장 성능을 낮출 수 있는지에 대한 정보를 줍니다. 딥러닝 같은 복잡한 모델에서 가장 적합한 데이터 오염 지점을 찾는 문제는 발전될 수 있습니다.

## 공격받은 데이터로 모델을 착각하게 만들기

사실. 데이터를 공격하는 것은 매우 쉽기 때문에 머신러닝 모델의 트레이닝 데이터까지 오염시킬 필요는 없습니다. 간단한 방해 데이터 추가를 통해서 모델이 즉시 실수를 할 수 있도록 할 수 있습니다! ( inference phase )

가장 일반적인 방해 데이터(Perturbation) 추가 방법은 adversarial example을 계산하는 겁니다. 사람이 구분할 수 없는 아주 작은 방해만 주더라도, 머신러닝 모델은 틀린 예측을 합니다. 

![images](http://cleverhans.io/assets/adversarial-example.png)

사람의 눈으로는 구분할 수 없지만, 모델의 예측을 바꾸기에는 충분한 것이 중요한 포인트입니다. 이 방해 데이터(Perturbation)은 모델의 예측 오차를 증가시키며 input domain의 특정 norm을 최소화하도록 계산됩니다. 이것은 효과적으로 정상적으로 분류되던 input data를 model's decision boundary로 밀어내 다른 class로 분류하도록 합니다. 

![images](http://cleverhans.io/assets/adversarial-example-crossing-decision-boundary.png)

많은 공격이 input perturbation 계산을 위해 모델 파라미터를 알고 있는 것으로 간주합니다. 이와는 대조적으로, 몇몇 연구들은 model의 prediction만 보고 공격을 하는 방법도 있습니다. 예를 들어, 공격자가 PageRank에 알맞도록 웹사이트를 디자인 하는 방법을 알거나, spam filter를 피할 수 있도록 만드는 것이죠. 

## 머신러닝에서의 사생활 문제